{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw29L6OveCUf",
        "outputId": "873006a1-c497-48b2-f6dd-6fff5a633997"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A8O93frKnZaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3556c2c-e9a7-4ca8-aee9-d7c1d02df8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial train dataset shape: (139995, 45)\n",
            "Initial validation dataset shape: (30005, 45)\n",
            "\n",
            "Missing ratio per column in the training set:\n",
            "ts                        0.000000\n",
            "src_ip                    0.000000\n",
            "src_port                  0.000000\n",
            "dst_ip                    0.000000\n",
            "dst_port                  0.000000\n",
            "proto                     0.000000\n",
            "service                   0.666360\n",
            "duration                  0.000000\n",
            "src_bytes                 0.000000\n",
            "dst_bytes                 0.000000\n",
            "conn_state                0.000000\n",
            "missed_bytes              0.000000\n",
            "src_pkts                  0.000000\n",
            "src_ip_bytes              0.000000\n",
            "dst_pkts                  0.000000\n",
            "dst_ip_bytes              0.000000\n",
            "dns_query                 0.832023\n",
            "dns_qclass                0.000000\n",
            "dns_qtype                 0.000000\n",
            "dns_rcode                 0.000000\n",
            "dns_AA                    0.830673\n",
            "dns_RD                    0.830673\n",
            "dns_RA                    0.830673\n",
            "dns_rejected              0.830673\n",
            "ssl_version               0.999386\n",
            "ssl_cipher                0.999386\n",
            "ssl_resumed               0.998857\n",
            "ssl_established           0.998857\n",
            "ssl_subject               0.999800\n",
            "ssl_issuer                0.999800\n",
            "http_trans_depth          0.999471\n",
            "http_method               0.999493\n",
            "http_uri                  0.999493\n",
            "http_referrer             1.000000\n",
            "http_version              0.999471\n",
            "http_request_body_len     0.000000\n",
            "http_response_body_len    0.000000\n",
            "http_status_code          0.000000\n",
            "http_user_agent           0.999493\n",
            "http_orig_mime_types      0.999957\n",
            "http_resp_mime_types      0.999729\n",
            "weird_name                0.998214\n",
            "weird_addl                0.998871\n",
            "weird_notice              0.998214\n",
            "label                     0.000000\n",
            "dtype: float64\n",
            "\n",
            "Columns to drop (more than 90% missing): ['service', 'dns_query', 'dns_AA', 'dns_RD', 'dns_RA', 'dns_rejected', 'ssl_version', 'ssl_cipher', 'ssl_resumed', 'ssl_established', 'ssl_subject', 'ssl_issuer', 'http_trans_depth', 'http_method', 'http_uri', 'http_referrer', 'http_version', 'http_user_agent', 'http_orig_mime_types', 'http_resp_mime_types', 'weird_name', 'weird_addl', 'weird_notice']\n",
            "\n",
            "Number of rows in training set before dropping columns: 139995\n",
            "Number of rows in validation set before dropping columns: 30005\n",
            "\n",
            "Train dataset shape after dropping columns: (139995, 22)\n",
            "Validation dataset shape after dropping columns: (30005, 22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e87594926b3d>:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_df.replace(\"-\", np.nan, inplace=True)\n",
            "<ipython-input-3-e87594926b3d>:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  val_df.replace(\"-\", np.nan, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load datasets from the specified paths\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Group5/Dataset2/train_70pct.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/Group5/Dataset2/validation_15pct.csv')\n",
        "\n",
        "# Drop the 'type' column if it exists (as per instructions)\n",
        "if 'type' in train_df.columns:\n",
        "    train_df.drop(columns=['type'], inplace=True)\n",
        "if 'type' in val_df.columns:\n",
        "    val_df.drop(columns=['type'], inplace=True)\n",
        "\n",
        "# Replace \"-\" with np.nan to mark meaningless values as missing\n",
        "train_df.replace(\"-\", np.nan, inplace=True)\n",
        "val_df.replace(\"-\", np.nan, inplace=True)\n",
        "\n",
        "# Print initial shape of the datasets\n",
        "print(\"Initial train dataset shape:\", train_df.shape)\n",
        "print(\"Initial validation dataset shape:\", val_df.shape)\n",
        "\n",
        "# Calculate missing data ratio for each column in the training set\n",
        "missing_ratio = train_df.isna().sum() / len(train_df)\n",
        "print(\"\\nMissing ratio per column in the training set:\")\n",
        "print(missing_ratio)\n",
        "\n",
        "# Identify columns with more than 90% missing values\n",
        "cols_to_drop = missing_ratio[missing_ratio > 0.6].index.tolist()\n",
        "print(\"\\nColumns to drop (more than 90% missing):\", cols_to_drop)\n",
        "\n",
        "# Before dropping columns, print number of rows (the row count remains the same)\n",
        "print(\"\\nNumber of rows in training set before dropping columns:\", train_df.shape[0])\n",
        "print(\"Number of rows in validation set before dropping columns:\", val_df.shape[0])\n",
        "\n",
        "# Drop columns with >90% missing values in both train and validation sets\n",
        "train_df.drop(columns=cols_to_drop, inplace=True)\n",
        "val_df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Print shapes after dropping columns to verify changes in number of columns\n",
        "print(\"\\nTrain dataset shape after dropping columns:\", train_df.shape)\n",
        "print(\"Validation dataset shape after dropping columns:\", val_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the count of missing values for each column in the training set\n",
        "missing_counts = train_df.isna().sum()\n",
        "\n",
        "# Filter to only columns with missing values\n",
        "cols_with_missing = missing_counts[missing_counts > 0]\n",
        "\n",
        "# Calculate missing ratio for each of these columns\n",
        "missing_ratio = cols_with_missing / len(train_df)\n",
        "\n",
        "print(\"Columns with missing values and their counts:\")\n",
        "print(cols_with_missing)\n",
        "print(\"\\nMissing ratio per column:\")\n",
        "print(missing_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmao2hXh4_RT",
        "outputId": "5f242c64-46a8-4f9b-d51b-0b720a64c5d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with missing values and their counts:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Missing ratio per column:\n",
            "Series([], dtype: float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the data types for each column in the training set\n",
        "print(\"Data types in the training set:\")\n",
        "print(train_df.dtypes)\n",
        "\n",
        "# Identify categorical features (typically those with object/string data type)\n",
        "categorical_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"\\nCategorical features in the training set:\")\n",
        "print(categorical_columns)\n",
        "\n",
        "# For each categorical column, print the top 10 unique values and their counts\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\nTop unique values in '{col}':\")\n",
        "    print(train_df[col].value_counts(dropna=False).head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl9BO2Kf-ovF",
        "outputId": "a1d992cf-0502-4111-8676-6764066c5a4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types in the training set:\n",
            "ts                          int64\n",
            "src_ip                     object\n",
            "src_port                    int64\n",
            "dst_ip                     object\n",
            "dst_port                    int64\n",
            "proto                      object\n",
            "duration                  float64\n",
            "src_bytes                  object\n",
            "dst_bytes                   int64\n",
            "conn_state                 object\n",
            "missed_bytes                int64\n",
            "src_pkts                    int64\n",
            "src_ip_bytes                int64\n",
            "dst_pkts                    int64\n",
            "dst_ip_bytes                int64\n",
            "dns_qclass                  int64\n",
            "dns_qtype                   int64\n",
            "dns_rcode                   int64\n",
            "http_request_body_len       int64\n",
            "http_response_body_len      int64\n",
            "http_status_code            int64\n",
            "label                       int64\n",
            "dtype: object\n",
            "\n",
            "Categorical features in the training set:\n",
            "['src_ip', 'dst_ip', 'proto', 'src_bytes', 'conn_state']\n",
            "\n",
            "Top unique values in 'src_ip':\n",
            "src_ip\n",
            "192.168.1.195    22356\n",
            "192.168.1.190    22163\n",
            "192.168.1.152    19332\n",
            "192.168.1.79     12079\n",
            "192.168.1.30      9567\n",
            "192.168.1.180     7218\n",
            "192.168.1.193     6628\n",
            "192.168.1.31      6335\n",
            "192.168.1.32      6150\n",
            "127.0.0.1         3876\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top unique values in 'dst_ip':\n",
            "dst_ip\n",
            "192.168.1.190    25579\n",
            "192.168.1.152    17196\n",
            "192.168.1.195    13251\n",
            "192.168.1.184     7801\n",
            "192.168.1.79      7291\n",
            "192.168.1.1       6161\n",
            "192.168.1.255     5410\n",
            "192.168.1.193     5259\n",
            "192.168.1.194     5200\n",
            "13.107.4.50       4929\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top unique values in 'proto':\n",
            "proto\n",
            "tcp     90701\n",
            "udp     47419\n",
            "icmp     1875\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top unique values in 'src_bytes':\n",
            "src_bytes\n",
            "0      96933\n",
            "130     2792\n",
            "132     1719\n",
            "226     1474\n",
            "383     1460\n",
            "48      1368\n",
            "41      1318\n",
            "39      1082\n",
            "66      1042\n",
            "31      1008\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top unique values in 'conn_state':\n",
            "conn_state\n",
            "S0        42144\n",
            "OTH       39427\n",
            "SF        31177\n",
            "REJ        7899\n",
            "SHR        7218\n",
            "S1         3730\n",
            "SH         3678\n",
            "S3         3358\n",
            "RSTOS0      599\n",
            "RSTO        409\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# High-cardinality columns to be label encoded: 'src_ip', 'dst_ip'\n",
        "high_card_cols = ['src_ip', 'dst_ip']\n",
        "\n",
        "for col in high_card_cols:\n",
        "    # Convert column to string (if not already) and create mapping from training set\n",
        "    train_df[col] = train_df[col].astype(str)\n",
        "    val_df[col] = val_df[col].astype(str)\n",
        "\n",
        "    # Create mapping dictionary based on training set unique values\n",
        "    unique_vals = train_df[col].unique()\n",
        "    mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
        "\n",
        "    # Apply mapping to the training set\n",
        "    train_df[col] = train_df[col].map(mapping)\n",
        "\n",
        "    # For the validation set, map the values and assign -1 for unseen values\n",
        "    val_df[col] = val_df[col].map(mapping).fillna(-1).astype(int)\n",
        "\n",
        "# One-hot encode low-cardinality columns using pd.get_dummies\n",
        "low_card_cols = ['proto', 'conn_state']\n",
        "train_df = pd.get_dummies(train_df, columns=low_card_cols, drop_first=True)\n",
        "val_df = pd.get_dummies(val_df, columns=low_card_cols, drop_first=True)\n",
        "\n",
        "# To ensure both train and validation have the same columns after one-hot encoding,\n",
        "# reindex the validation set based on the training set's columns\n",
        "val_df = val_df.reindex(columns=train_df.columns, fill_value=0)\n",
        "\n",
        "# Display the new shape and a few rows to confirm encoding\n",
        "print(\"Train dataset shape after encoding:\", train_df.shape)\n",
        "print(\"Validation dataset shape after encoding:\", val_df.shape)\n",
        "print(\"\\nSample of processed training data:\")\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EblcBxX8Hq8Z",
        "outputId": "a57e91f9-f178-446d-96c2-894b16790193"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape after encoding: (139995, 34)\n",
            "Validation dataset shape after encoding: (30005, 34)\n",
            "\n",
            "Sample of processed training data:\n",
            "           ts  src_ip  src_port  dst_ip  dst_port  duration src_bytes  \\\n",
            "0  1556027889       0     15544       0     64609  0.000130         0   \n",
            "1  1556311612       1     43674       1        21  0.000149         0   \n",
            "2  1556327860       2     50463       2      1900  0.618052       375   \n",
            "3  1556548159       3     49754       3     41952  0.015973       201   \n",
            "4  1556426221       4     49189       4      1880  0.000000         0   \n",
            "\n",
            "   dst_bytes  missed_bytes  src_pkts  ...  conn_state_RSTOS0  conn_state_RSTR  \\\n",
            "0          0             0         1  ...              False            False   \n",
            "1          0             0         1  ...              False            False   \n",
            "2          0             0         3  ...              False            False   \n",
            "3          0             0        22  ...              False            False   \n",
            "4          0             0         1  ...              False            False   \n",
            "\n",
            "   conn_state_RSTRH  conn_state_S0  conn_state_S1  conn_state_S2  \\\n",
            "0             False          False          False          False   \n",
            "1             False          False          False          False   \n",
            "2             False           True          False          False   \n",
            "3             False          False          False          False   \n",
            "4             False          False          False          False   \n",
            "\n",
            "   conn_state_S3  conn_state_SF  conn_state_SH  conn_state_SHR  \n",
            "0          False          False          False           False  \n",
            "1          False          False          False           False  \n",
            "2          False          False          False           False  \n",
            "3          False          False           True           False  \n",
            "4          False          False          False           False  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the continuous features that need scaling\n",
        "continuous_cols = [\n",
        "    'ts', 'src_port', 'dst_port', 'duration', 'src_bytes', 'dst_bytes',\n",
        "    'missed_bytes', 'src_pkts', 'src_ip_bytes', 'dst_pkts', 'dst_ip_bytes',\n",
        "    'dns_qclass', 'dns_qtype', 'dns_rcode', 'http_request_body_len',\n",
        "    'http_response_body_len', 'http_status_code'\n",
        "]\n",
        "\n",
        "# Ensure that continuous columns are numeric by converting them\n",
        "for col in continuous_cols:\n",
        "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "    val_df[col] = pd.to_numeric(val_df[col], errors='coerce')   # Added this to perform same transformation as training set\n",
        "\n",
        "# Re-run outlier analysis\n",
        "def calculate_outliers(df, column):\n",
        "    # Calculate the 25th and 75th percentiles\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Identify outliers\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return lower_bound, upper_bound, len(outliers), len(df)\n",
        "\n",
        "print(\"Outlier Analysis using IQR method:\")\n",
        "for col in continuous_cols:\n",
        "    lower, upper, outlier_count, total_count = calculate_outliers(train_df, col)\n",
        "    percent_outliers = (outlier_count / total_count) * 100\n",
        "    print(f\"Column: {col}\")\n",
        "    print(f\"  Lower Bound: {lower:.2f}, Upper Bound: {upper:.2f}\")\n",
        "    print(f\"  Outliers: {outlier_count} out of {total_count} ({percent_outliers:.2f}%)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvlfuEPARqd2",
        "outputId": "8e4bf511-e1d7-4562-bd64-70b1abf4621c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier Analysis using IQR method:\n",
            "Column: ts\n",
            "  Lower Bound: 1555436379.75, Upper Bound: 1557015997.75\n",
            "  Outliers: 28186 out of 139995 (20.13%)\n",
            "\n",
            "Column: src_port\n",
            "  Lower Bound: -55197.25, Upper Bound: 116520.75\n",
            "  Outliers: 0 out of 139995 (0.00%)\n",
            "\n",
            "Column: dst_port\n",
            "  Lower Bound: -15553.00, Upper Bound: 26135.00\n",
            "  Outliers: 24015 out of 139995 (17.15%)\n",
            "\n",
            "Column: duration\n",
            "  Lower Bound: -0.07, Upper Bound: 0.12\n",
            "  Outliers: 32410 out of 139995 (23.15%)\n",
            "\n",
            "Column: src_bytes\n",
            "  Lower Bound: -64.50, Upper Bound: 107.50\n",
            "  Outliers: 23580 out of 139995 (16.84%)\n",
            "\n",
            "Column: dst_bytes\n",
            "  Lower Bound: -111.00, Upper Bound: 185.00\n",
            "  Outliers: 27327 out of 139995 (19.52%)\n",
            "\n",
            "Column: missed_bytes\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 650 out of 139995 (0.46%)\n",
            "\n",
            "Column: src_pkts\n",
            "  Lower Bound: -2.00, Upper Bound: 6.00\n",
            "  Outliers: 19708 out of 139995 (14.08%)\n",
            "\n",
            "Column: src_ip_bytes\n",
            "  Lower Bound: -480.50, Upper Bound: 907.50\n",
            "  Outliers: 20585 out of 139995 (14.70%)\n",
            "\n",
            "Column: dst_pkts\n",
            "  Lower Bound: -1.50, Upper Bound: 2.50\n",
            "  Outliers: 17810 out of 139995 (12.72%)\n",
            "\n",
            "Column: dst_ip_bytes\n",
            "  Lower Bound: -339.00, Upper Bound: 565.00\n",
            "  Outliers: 24530 out of 139995 (17.52%)\n",
            "\n",
            "Column: dns_qclass\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 23355 out of 139995 (16.68%)\n",
            "\n",
            "Column: dns_qtype\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 23355 out of 139995 (16.68%)\n",
            "\n",
            "Column: dns_rcode\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 4722 out of 139995 (3.37%)\n",
            "\n",
            "Column: http_request_body_len\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 6 out of 139995 (0.00%)\n",
            "\n",
            "Column: http_response_body_len\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 47 out of 139995 (0.03%)\n",
            "\n",
            "Column: http_status_code\n",
            "  Lower Bound: 0.00, Upper Bound: 0.00\n",
            "  Outliers: 74 out of 139995 (0.05%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Initialize RobustScaler\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "# Fit the scaler on the training data for the continuous columns\n",
        "train_df[continuous_cols] = robust_scaler.fit_transform(train_df[continuous_cols])\n",
        "\n",
        "# Transform the validation data using the same scaler\n",
        "val_df[continuous_cols] = robust_scaler.transform(val_df[continuous_cols])\n",
        "\n",
        "# Display summary statistics for the scaled continuous features in the training set\n",
        "print(\"Summary statistics for scaled continuous features (training set) using RobustScaler:\")\n",
        "print(train_df[continuous_cols].describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOjuwx_PTH47",
        "outputId": "d63d28cb-1bb3-416a-d6cc-ad01950695e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics for scaled continuous features (training set) using RobustScaler:\n",
            "                  ts       src_port       dst_port      duration  \\\n",
            "count  139995.000000  139995.000000  139995.000000  1.399950e+05   \n",
            "mean       -0.844077      -0.186926       0.883975  1.262646e+02   \n",
            "std         2.045427       0.498449       1.685476  5.441389e+03   \n",
            "min        -5.089552      -1.013988      -0.182307 -4.522646e-04   \n",
            "25%        -0.454562      -0.799753      -0.174631 -4.522646e-04   \n",
            "50%         0.000000       0.000000       0.000000  0.000000e+00   \n",
            "75%         0.545438       0.200247       0.825369  9.995477e-01   \n",
            "max         0.863275       0.512561       6.105258  1.734747e+06   \n",
            "\n",
            "          src_bytes     dst_bytes  missed_bytes       src_pkts   src_ip_bytes  \\\n",
            "count  1.398890e+05  1.399950e+05  1.399950e+05  139995.000000  139995.000000   \n",
            "mean   4.101174e+03  2.454458e+03  7.712455e+03      10.895193       7.031493   \n",
            "std    3.079419e+05  2.216291e+05  1.406378e+06     878.318715     557.173885   \n",
            "min    0.000000e+00  0.000000e+00  0.000000e+00      -0.500000      -0.198847   \n",
            "25%    0.000000e+00  0.000000e+00  0.000000e+00       0.000000      -0.083573   \n",
            "50%    0.000000e+00  0.000000e+00  0.000000e+00       0.000000       0.000000   \n",
            "75%    1.000000e+00  1.000000e+00  0.000000e+00       1.000000       0.916427   \n",
            "max    4.895823e+07  5.785445e+07  4.080989e+08  294013.000000  104320.181556   \n",
            "\n",
            "           dst_pkts   dst_ip_bytes     dns_qclass      dns_qtype  \\\n",
            "count  1.399950e+05  139995.000000  139995.000000  139995.000000   \n",
            "mean   3.828887e+01      19.819351      82.557884       4.626172   \n",
            "std    8.949876e+03    1962.064989    1641.084560      26.469775   \n",
            "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
            "25%    0.000000e+00       0.000000       0.000000       0.000000   \n",
            "50%    0.000000e+00       0.000000       0.000000       0.000000   \n",
            "75%    1.000000e+00       1.000000       0.000000       0.000000   \n",
            "max    3.238855e+06  426838.331858   32769.000000     255.000000   \n",
            "\n",
            "           dns_rcode  http_request_body_len  http_response_body_len  \\\n",
            "count  139995.000000          139995.000000            1.399950e+05   \n",
            "mean        0.103175               0.005507            2.463234e+01   \n",
            "std         0.558094               1.156790            4.862566e+03   \n",
            "min         0.000000               0.000000            0.000000e+00   \n",
            "25%         0.000000               0.000000            0.000000e+00   \n",
            "50%         0.000000               0.000000            0.000000e+00   \n",
            "75%         0.000000               0.000000            0.000000e+00   \n",
            "max         5.000000             300.000000            1.048576e+06   \n",
            "\n",
            "       http_status_code  \n",
            "count     139995.000000  \n",
            "mean           0.127933  \n",
            "std            5.678734  \n",
            "min            0.000000  \n",
            "25%            0.000000  \n",
            "50%            0.000000  \n",
            "75%            0.000000  \n",
            "max          304.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column for the training set\n",
        "print(\"Missing values in the training set:\")\n",
        "missing_counts_train = train_df.isnull().sum()\n",
        "print(missing_counts_train)\n",
        "\n",
        "# Check for missing values in each column for the validation set\n",
        "print(\"\\nMissing values in the validation set:\")\n",
        "missing_counts_val = val_df.isnull().sum()\n",
        "print(missing_counts_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB69eX1PZV7I",
        "outputId": "d78e5b63-f400-47bd-8b03-8698ff7e62b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in the training set:\n",
            "ts                          0\n",
            "src_ip                      0\n",
            "src_port                    0\n",
            "dst_ip                      0\n",
            "dst_port                    0\n",
            "duration                    0\n",
            "src_bytes                 106\n",
            "dst_bytes                   0\n",
            "missed_bytes                0\n",
            "src_pkts                    0\n",
            "src_ip_bytes                0\n",
            "dst_pkts                    0\n",
            "dst_ip_bytes                0\n",
            "dns_qclass                  0\n",
            "dns_qtype                   0\n",
            "dns_rcode                   0\n",
            "http_request_body_len       0\n",
            "http_response_body_len      0\n",
            "http_status_code            0\n",
            "label                       0\n",
            "proto_tcp                   0\n",
            "proto_udp                   0\n",
            "conn_state_REJ              0\n",
            "conn_state_RSTO             0\n",
            "conn_state_RSTOS0           0\n",
            "conn_state_RSTR             0\n",
            "conn_state_RSTRH            0\n",
            "conn_state_S0               0\n",
            "conn_state_S1               0\n",
            "conn_state_S2               0\n",
            "conn_state_S3               0\n",
            "conn_state_SF               0\n",
            "conn_state_SH               0\n",
            "conn_state_SHR              0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in the validation set:\n",
            "ts                         0\n",
            "src_ip                     0\n",
            "src_port                   0\n",
            "dst_ip                     0\n",
            "dst_port                   0\n",
            "duration                   0\n",
            "src_bytes                 26\n",
            "dst_bytes                  0\n",
            "missed_bytes               0\n",
            "src_pkts                   0\n",
            "src_ip_bytes               0\n",
            "dst_pkts                   0\n",
            "dst_ip_bytes               0\n",
            "dns_qclass                 0\n",
            "dns_qtype                  0\n",
            "dns_rcode                  0\n",
            "http_request_body_len      0\n",
            "http_response_body_len     0\n",
            "http_status_code           0\n",
            "label                      0\n",
            "proto_tcp                  0\n",
            "proto_udp                  0\n",
            "conn_state_REJ             0\n",
            "conn_state_RSTO            0\n",
            "conn_state_RSTOS0          0\n",
            "conn_state_RSTR            0\n",
            "conn_state_RSTRH           0\n",
            "conn_state_S0              0\n",
            "conn_state_S1              0\n",
            "conn_state_S2              0\n",
            "conn_state_S3              0\n",
            "conn_state_SF              0\n",
            "conn_state_SH              0\n",
            "conn_state_SHR             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the original training dataset\n",
        "orig_train_df = pd.read_csv('/content/drive/MyDrive/Group5/Dataset2/train_70pct.csv')\n",
        "\n",
        "# Replace \"-\" with np.nan as before\n",
        "orig_train_df.replace(\"-\", np.nan, inplace=True)\n",
        "\n",
        "# Attempt to convert the 'src_bytes' column to numeric; non-convertible values will become NaN\n",
        "converted_src_bytes = pd.to_numeric(orig_train_df['src_bytes'], errors='coerce')\n",
        "\n",
        "# Identify rows where conversion resulted in NaN but the original value is not NaN\n",
        "problem_rows = orig_train_df[pd.isna(converted_src_bytes) & orig_train_df['src_bytes'].notna()]\n",
        "\n",
        "# Display the problematic rows and the original values in 'src_bytes'\n",
        "print(\"Rows with problematic 'src_bytes' values:\")\n",
        "print(problem_rows[['src_bytes']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyZy3T7LaYaj",
        "outputId": "60bc061c-0df1-459f-dfce-eb9a94dc7af0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with problematic 'src_bytes' values:\n",
            "       src_bytes\n",
            "2010     0.0.0.0\n",
            "2043     0.0.0.0\n",
            "7552     0.0.0.0\n",
            "11137    0.0.0.0\n",
            "11919    0.0.0.0\n",
            "...          ...\n",
            "127332   0.0.0.0\n",
            "131910   0.0.0.0\n",
            "135663   0.0.0.0\n",
            "136459   0.0.0.0\n",
            "136600   0.0.0.0\n",
            "\n",
            "[106 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9c13abbb7c11>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  orig_train_df.replace(\"-\", np.nan, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Separate features and target (if not already done)\n",
        "X_train = train_df.drop(columns=['label'])\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_val = val_df.drop(columns=['label'])\n",
        "y_val = val_df['label']\n",
        "\n",
        "# Build a pipeline that first imputes missing values then fits LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values with median\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Set up a stratified 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "\n",
        "# Define the parameter grid for the pipeline\n",
        "param_grid = {\n",
        "    'lr__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'lr__penalty': ['l2'],  # using l2 penalty\n",
        "    'lr__solver': ['lbfgs'],  # compatible with l2 penalty\n",
        "    'lr__class_weight': [None, 'balanced']  # handle class imbalance if needed\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with F1-score as the scoring metric\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best CV score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV F1 score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_pred = grid_search.predict(X_val)\n",
        "print(\"\\nValidation set classification report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the best Logistic Regression model\n",
        "joblib.dump(grid_search.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/logistic_regression_model.pkl')\n",
        "print(\"Logistic Regression model saved as logistic_regression_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue7bHugLXy6Z",
        "outputId": "b2743dd2-de66-4d6a-be82-52d9f855c858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "joblib.dump(grid_search.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/logistic_regression_model.pkl')"
      ],
      "metadata": {
        "id": "5WCJRwchymML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Build a pipeline for LightGBM\n",
        "pipeline_lgbm = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # impute missing values with median\n",
        "    ('lgbm', LGBMClassifier(random_state=89))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for LightGBM\n",
        "param_grid_lgbm = {\n",
        "    'lgbm__num_leaves': [31, 50],\n",
        "    'lgbm__learning_rate': [0.1, 0.05],\n",
        "    'lgbm__n_estimators': [100, 200],\n",
        "    'lgbm__class_weight': [None, 'balanced']  # helps if there is class imbalance\n",
        "}\n",
        "\n",
        "# Set up stratified 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "\n",
        "# Initialize GridSearchCV with F1-score as the scoring metric\n",
        "grid_search_lgbm = GridSearchCV(pipeline_lgbm, param_grid_lgbm, cv=skf, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best CV score\n",
        "print(\"Best parameters for LightGBM:\", grid_search_lgbm.best_params_)\n",
        "print(\"Best CV F1 score for LightGBM:\", grid_search_lgbm.best_score_)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_pred_lgbm = grid_search_lgbm.predict(X_val)\n",
        "print(\"\\nValidation set classification report for LightGBM:\")\n",
        "print(classification_report(y_val, y_pred_lgbm))\n",
        "\n",
        "\n",
        "# Save the best LightGBM model\n",
        "joblib.dump(grid_search_lgbm.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/lightgbm_model.pkl')\n",
        "print(\"LightGBM model saved as lightgbm_model.pkl\")\n"
      ],
      "metadata": {
        "id": "2wnyUoQMjYbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Build a pipeline for Gaussian Naive Bayes\n",
        "pipeline_nb = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # impute missing values with median\n",
        "    ('nb', GaussianNB())\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GaussianNB (var_smoothing controls stability)\n",
        "param_grid_nb = {\n",
        "    'nb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "}\n",
        "\n",
        "# Set up stratified 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "\n",
        "# Initialize GridSearchCV with F1-score as the scoring metric\n",
        "grid_search_nb = GridSearchCV(pipeline_nb, param_grid_nb, cv=skf, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search_nb.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best CV score\n",
        "print(\"Best parameters for Gaussian Naive Bayes:\", grid_search_nb.best_params_)\n",
        "print(\"Best CV F1 score for Gaussian Naive Bayes:\", grid_search_nb.best_score_)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_pred_nb = grid_search_nb.predict(X_val)\n",
        "print(\"\\nValidation set classification report for Gaussian Naive Bayes:\")\n",
        "print(classification_report(y_val, y_pred_nb))\n",
        "\n",
        "# Save the best Gaussian Naive Bayes model using joblib\n",
        "joblib.dump(grid_search_nb.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/gaussian_nb_model.pkl')\n",
        "print(\"Gaussian Naive Bayes model saved as gaussian_nb_model.pkl\")\n"
      ],
      "metadata": {
        "id": "_wNrtCFZsJwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Build a pipeline for MLPClassifier\n",
        "pipeline_mlp = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with the median\n",
        "    ('mlp', MLPClassifier(max_iter=500, random_state=89))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for MLPClassifier\n",
        "param_grid_mlp = {\n",
        "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'mlp__alpha': [0.0001, 0.001, 0.01],\n",
        "    'mlp__activation': ['relu', 'tanh'],\n",
        "    'mlp__solver': ['adam']  # 'adam' is a good default solver for neural networks\n",
        "}\n",
        "\n",
        "# Set up stratified 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "\n",
        "# Initialize GridSearchCV with F1-score as the scoring metric\n",
        "grid_search_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=skf, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best CV score\n",
        "print(\"Best parameters for MLP:\", grid_search_mlp.best_params_)\n",
        "print(\"Best CV F1 score for MLP:\", grid_search_mlp.best_score_)\n",
        "\n",
        "# Evaluate the best MLP model on the validation set\n",
        "y_pred_mlp = grid_search_mlp.predict(X_val)\n",
        "print(\"\\nValidation set classification report for MLP:\")\n",
        "print(classification_report(y_val, y_pred_mlp))\n",
        "\n",
        "# Save the best MLP model using joblib\n",
        "joblib.dump(grid_search_mlp.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/mlp_model.pkl')\n",
        "print(\"MLP model saved as mlp_model.pkl\")\n"
      ],
      "metadata": {
        "id": "a0amY4h8sOKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Build a pipeline for SVM\n",
        "pipeline_svm = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Ensure no missing values remain\n",
        "    ('svm', SVC(random_state=89))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for SVM\n",
        "param_grid_svm = {\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__kernel': ['rbf'],       # Use RBF kernel for non-linear decision boundaries\n",
        "    'svm__gamma': ['scale', 'auto'],\n",
        "    'svm__class_weight': [None, 'balanced']  # Handle potential class imbalance\n",
        "}\n",
        "\n",
        "# Set up stratified 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "\n",
        "# Initialize GridSearchCV using F1-score as the scoring metric\n",
        "grid_search_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=skf, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best CV score\n",
        "print(\"Best parameters for SVM:\", grid_search_svm.best_params_)\n",
        "print(\"Best CV F1 score for SVM:\", grid_search_svm.best_score_)\n",
        "\n",
        "# Evaluate the best SVM model on the validation set\n",
        "y_pred_svm = grid_search_svm.predict(X_val)\n",
        "print(\"\\nValidation set classification report for SVM:\")\n",
        "print(classification_report(y_val, y_pred_svm))\n",
        "\n",
        "# Save the best SVM model using joblib\n",
        "joblib.dump(grid_search_svm.best_estimator_, '/content/drive/MyDrive/Group5/Dataset2/svm_model.pkl')\n",
        "print(\"SVM model saved as svm_model.pkl\")\n"
      ],
      "metadata": {
        "id": "X83cteDxszyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Dictionary to store results for each model\n",
        "results = {}\n",
        "\n",
        "# List of models and their corresponding grid search objects\n",
        "models = {\n",
        "    'Logistic Regression': grid_search,\n",
        "    'LightGBM': grid_search_lgbm,\n",
        "    'Gaussian Naive Bayes': grid_search_nb,\n",
        "    'MLP': grid_search_mlp,\n",
        "    'SVM': grid_search_svm\n",
        "}\n",
        "\n",
        "# Calculate metrics for each model\n",
        "for name, grid in models.items():\n",
        "    # Best cross-validation F1 score from grid search\n",
        "    best_cv_f1 = grid.best_score_\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = grid.predict(X_val)\n",
        "    # Calculate the validation F1 score (assuming binary classification)\n",
        "    val_f1 = f1_score(y_val, y_pred, average='binary')\n",
        "\n",
        "    # Store the results along with the best parameters found\n",
        "    results[name] = {\n",
        "        'Best CV F1': best_cv_f1,\n",
        "        'Validation F1': val_f1,\n",
        "        'Best Params': grid.best_params_\n",
        "    }\n",
        "\n",
        "# Convert results into a DataFrame for easier comparison\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "results_df.sort_values(by='Validation F1', ascending=False, inplace=True)\n",
        "\n",
        "print(\"Model Ranking based on Validation F1 Score:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "EWTA--nEwa-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}