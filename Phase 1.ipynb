{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Preparing Dataset with same distribution as the complete dataset"],"metadata":{"id":"RQqHRuahrsA1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaPivA2PW3FH","executionInfo":{"status":"ok","timestamp":1741213533189,"user_tz":300,"elapsed":60335,"user":{"displayName":"Sadra Teymourian","userId":"07534993455976504565"}},"outputId":"3c261196-b5ec-4e1a-8d9f-184ec5f3a243"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/Group5/Data/Network_dataset_1.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_10.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_11.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_13.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_14.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_12.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_16.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_17.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_15.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_18.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_19.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_20.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_2.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_22.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_23.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_21.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_3.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_5.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_4.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_6.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_7.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_8.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_9.csv ...\n","Overall label distribution:\n","{1: 21542641, 0: 796380}\n"]}],"source":["import pandas as pd\n","import os\n","from glob import glob\n","\n","# Define the folder path where your CSV files are stored\n","data_folder = \"/content/drive/MyDrive/Group5/Data\"\n","\n","# Get a list of CSV files matching the pattern (adjust the pattern if needed)\n","csv_files = glob(os.path.join(data_folder, \"Network_dataset_*.csv\"))\n","\n","# Dictionary to hold the total counts of each label\n","label_counts_total = {}\n","\n","# Loop over each CSV file\n","for file in csv_files:\n","    print(f\"Processing {file} ...\")\n","    try:\n","        # Read only the 'label' column; change 'label' if your column has a different name\n","        df = pd.read_csv(file, usecols=[\"label\"])\n","    except Exception as e:\n","        print(f\"Error reading {file}: {e}\")\n","        continue\n","\n","    # Count label occurrences in the current file\n","    counts = df[\"label\"].value_counts().to_dict()\n","\n","    # Update the overall counts\n","    for label, count in counts.items():\n","        label_counts_total[label] = label_counts_total.get(label, 0) + count\n","\n","# Display the overall label distribution\n","print(\"Overall label distribution:\")\n","print(label_counts_total)\n"]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from glob import glob\n","\n","# Define the folder path where your CSV files are stored\n","data_folder = \"/content/drive/MyDrive/Group5/Data\"\n","\n","# Get a list of CSV files matching the pattern\n","csv_files = glob(os.path.join(data_folder, \"Network_dataset_*.csv\"))\n","\n","# Dictionary to hold the total counts of each type\n","type_counts_total = {}\n","\n","# Loop over each CSV file\n","for file in csv_files:\n","    print(f\"Processing {file} ...\")\n","    try:\n","        # Read only the 'type' column; adjust the column name if necessary\n","        df = pd.read_csv(file, usecols=[\"type\"])\n","    except Exception as e:\n","        print(f\"Error reading {file}: {e}\")\n","        continue\n","\n","    # Count type occurrences in the current file\n","    counts = df[\"type\"].value_counts().to_dict()\n","\n","    # Update the overall counts\n","    for t, count in counts.items():\n","        type_counts_total[t] = type_counts_total.get(t, 0) + count\n","\n","# Display the overall type distribution\n","print(\"Overall 'type' distribution:\")\n","print(type_counts_total)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBpfT8gwjBBS","executionInfo":{"status":"ok","timestamp":1741215875160,"user_tz":300,"elapsed":73530,"user":{"displayName":"Sadra Teymourian","userId":"07534993455976504565"}},"outputId":"40ce9495-6d1c-43e8-9a8c-8c1f5914ff40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/Group5/Data/Network_dataset_1.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_10.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_11.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_13.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_14.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_12.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_16.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_17.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_15.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_18.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_19.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_20.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_2.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_22.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_23.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_21.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_3.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_5.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_4.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_6.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_7.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_8.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_9.csv ...\n","Overall 'type' distribution:\n","{'scanning': 7140161, 'normal': 796380, 'dos': 3375328, 'injection': 452659, 'ddos': 6165008, 'password': 1718568, 'xss': 2108944, 'backdoor': 508116, 'ransomware': 72805, 'mitm': 1052}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from glob import glob\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Define the folder path where your CSV files are stored\n","data_folder = \"/content/drive/MyDrive/Group5/Data\"\n","csv_files = glob(os.path.join(data_folder, \"Network_dataset_*.csv\"))\n","\n","# --- Step 1: Find the common columns across all CSV files ---\n","common_columns = None\n","for file in csv_files:\n","    try:\n","        # Read only header of the CSV file\n","        df_header = pd.read_csv(file, nrows=0)\n","        file_columns = set(df_header.columns)\n","        if common_columns is None:\n","            common_columns = file_columns\n","        else:\n","            common_columns = common_columns.intersection(file_columns)\n","    except Exception as e:\n","        print(f\"Error reading header from {file}: {e}\")\n","\n","common_columns = list(common_columns)\n","print(\"Common columns across all CSV files:\")\n","print(common_columns)\n","\n","# --- Step 2: Sample rows from each file using only the common columns ---\n","sample_list = []\n","chunksize = 100000\n","target_rows = 1000000\n","# Calculate sampling fraction based on approximate total rows (adjust as needed)\n","# Here, using an estimate of 22,339,021 rows in the full dataset.\n","sampling_frac = target_rows / 22339021\n","print(f\"Using a sampling fraction of {sampling_frac:.4f} per chunk.\")\n","\n","for file in csv_files:\n","    print(f\"Processing {file} ...\")\n","    try:\n","        for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n","            # Sample a fraction of rows from the chunk with a fixed seed for reproducibility\n","            sample_chunk = chunk.sample(frac=sampling_frac, random_state=42)\n","            sample_list.append(sample_chunk)\n","    except Exception as e:\n","        print(f\"Error processing {file}: {e}\")\n","\n","# Combine all sampled chunks\n","sample_df = pd.concat(sample_list, ignore_index=True)\n","print(f\"Initial sampled rows: {len(sample_df)}\")\n","\n","# If the sampled rows are less than the target, adjust target_rows\n","if len(sample_df) < target_rows:\n","    print(f\"Sampled rows ({len(sample_df)}) are less than target ({target_rows}). Using all sampled rows.\")\n","    target_rows = len(sample_df)\n","\n","# --- Step 3: Stratified Sampling to get exactly target_rows if needed ---\n","if len(sample_df) == target_rows:\n","    subset_df = sample_df.copy()\n","else:\n","    # We assume that the common columns include 'type' and 'label' needed for stratification.\n","    # Create a helper column combining 'type' and 'label'\n","    sample_df[\"stratify_group\"] = sample_df[\"type\"].astype(str) + \"_\" + sample_df[\"label\"].astype(str)\n","\n","    sss = StratifiedShuffleSplit(n_splits=1, train_size=target_rows, random_state=42)\n","    for train_index, _ in sss.split(sample_df, sample_df[\"stratify_group\"]):\n","        subset_df = sample_df.iloc[train_index].copy()\n","    # Drop the helper column after sampling\n","    subset_df = subset_df.drop(columns=[\"stratify_group\"])\n","\n","print(f\"Final subset rows: {len(subset_df)}\")\n","\n","# --- Step 4: Save the subset without extra index or columns ---\n","subset_df.to_csv(\"subset_1M.csv\", index=False)\n","print(\"Subset saved to 'subset_1M.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2vi7AAzOQNp","executionInfo":{"status":"ok","timestamp":1741232457219,"user_tz":300,"elapsed":142169,"user":{"displayName":"Sadra Teymourian","userId":"07534993455976504565"}},"outputId":"d16a4c78-7d75-4a6e-99a9-c6b4d89bbfb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Common columns across all CSV files:\n","['conn_state', 'weird_addl', 'http_orig_mime_types', 'dst_bytes', 'http_trans_depth', 'http_status_code', 'duration', 'src_pkts', 'dns_qclass', 'dst_ip', 'http_referrer', 'http_user_agent', 'dns_AA', 'dns_RA', 'type', 'dst_ip_bytes', 'src_ip', 'http_uri', 'http_method', 'dst_port', 'ssl_cipher', 'src_bytes', 'missed_bytes', 'ssl_established', 'service', 'http_response_body_len', 'dns_RD', 'http_version', 'label', 'http_request_body_len', 'ssl_subject', 'dst_pkts', 'dns_rejected', 'src_port', 'dns_rcode', 'ts', 'dns_query', 'dns_qtype', 'ssl_version', 'http_resp_mime_types', 'ssl_resumed', 'weird_notice', 'weird_name', 'ssl_issuer', 'proto', 'src_ip_bytes']\n","Using a sampling fraction of 0.0448 per chunk.\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_1.csv ...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n","<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n"]},{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/Group5/Data/Network_dataset_10.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_11.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_13.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_14.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_12.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_16.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_17.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_15.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_18.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_19.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_20.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_2.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_22.csv ...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n","<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n","<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n"]},{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/Group5/Data/Network_dataset_23.csv ...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-7474179ade0e>:40: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(file, usecols=common_columns, chunksize=chunksize):\n"]},{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/Group5/Data/Network_dataset_21.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_3.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_5.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_4.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_6.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_7.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_8.csv ...\n","Processing /content/drive/MyDrive/Group5/Data/Network_dataset_9.csv ...\n","Initial sampled rows: 999895\n","Sampled rows (999895) are less than target (1000000). Using all sampled rows.\n","Final subset rows: 999895\n","Subset saved to 'subset_1M.csv'.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Read the subset from the CSV file\n","subset_df = pd.read_csv(\"subset_1M.csv\")\n","\n","# Create a helper column for stratification that combines 'type' and 'label'\n","subset_df[\"stratify_group\"] = subset_df[\"type\"].astype(str) + \"_\" + subset_df[\"label\"].astype(str)\n","\n","# Split into 20% labeled and 80% unlabeled portions using stratification\n","labeled_df, unlabeled_df = train_test_split(\n","    subset_df,\n","    test_size=0.8,   # 80% of the data will be unlabeled\n","    stratify=subset_df[\"stratify_group\"],\n","    random_state=42\n",")\n","\n","# Drop the helper column as it's no longer needed\n","labeled_df = labeled_df.drop(columns=[\"stratify_group\"])\n","unlabeled_df = unlabeled_df.drop(columns=[\"stratify_group\"])\n","\n","# Function to print counts with percentages\n","def print_distribution(df, column_name):\n","    counts = df[column_name].value_counts()\n","    percentages = df[column_name].value_counts(normalize=True) * 100\n","    for category in counts.index:\n","        print(f\"{category}: {counts[category]} ({percentages[category]:.2f}%)\")\n","\n","# Display information for the labeled portion\n","print(\"Labeled portion shape:\", labeled_df.shape)\n","print(\"\\nLabeled portion distribution (label counts):\")\n","print_distribution(labeled_df, \"label\")\n","print(\"\\nLabeled portion distribution (type counts):\")\n","print_distribution(labeled_df, \"type\")\n","\n","# Display information for the unlabeled portion\n","print(\"\\nUnlabeled portion shape:\", unlabeled_df.shape)\n","print(\"\\nUnlabeled portion distribution (label counts):\")\n","print_distribution(unlabeled_df, \"label\")\n","print(\"\\nUnlabeled portion distribution (type counts):\")\n","print_distribution(unlabeled_df, \"type\")\n","\n","# Optionally, save the splits for further processing\n","labeled_df.to_csv(\"labeled_20pct.csv\", index=False)\n","unlabeled_df.to_csv(\"unlabeled_80pct.csv\", index=False)\n","print(\"\\nSplits saved as 'labeled_20pct.csv' and 'unlabeled_80pct.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stIYXcWPmcEw","executionInfo":{"status":"ok","timestamp":1741233662688,"user_tz":300,"elapsed":25456,"user":{"displayName":"Sadra Teymourian","userId":"07534993455976504565"}},"outputId":"f9cc9374-78b5-4a0c-e7ce-c13256bad16d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-f2ceee780704>:5: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  subset_df = pd.read_csv(\"subset_1M.csv\")\n"]},{"output_type":"stream","name":"stdout","text":["Labeled portion shape: (199979, 46)\n","\n","Labeled portion distribution (label counts):\n","1: 192786 (96.40%)\n","0: 7193 (3.60%)\n","\n","Labeled portion distribution (type counts):\n","scanning: 63915 (31.96%)\n","ddos: 55159 (27.58%)\n","dos: 30188 (15.10%)\n","xss: 18865 (9.43%)\n","password: 15388 (7.69%)\n","normal: 7193 (3.60%)\n","backdoor: 4545 (2.27%)\n","injection: 4058 (2.03%)\n","ransomware: 658 (0.33%)\n","mitm: 10 (0.01%)\n","\n","Unlabeled portion shape: (799916, 46)\n","\n","Unlabeled portion distribution (label counts):\n","1: 771145 (96.40%)\n","0: 28771 (3.60%)\n","\n","Unlabeled portion distribution (type counts):\n","scanning: 255661 (31.96%)\n","ddos: 220637 (27.58%)\n","dos: 120754 (15.10%)\n","xss: 75459 (9.43%)\n","password: 61550 (7.69%)\n","normal: 28771 (3.60%)\n","backdoor: 18180 (2.27%)\n","injection: 16234 (2.03%)\n","ransomware: 2632 (0.33%)\n","mitm: 38 (0.00%)\n","\n","Splits saved as 'labeled_20pct.csv' and 'unlabeled_80pct.csv'.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Read the labeled portion from the CSV file\n","labeled_df = pd.read_csv(\"labeled_20pct.csv\")\n","\n","# Create a helper column for stratification combining 'type' and 'label'\n","labeled_df[\"stratify_group\"] = labeled_df[\"type\"].astype(str) + \"_\" + labeled_df[\"label\"].astype(str)\n","\n","# Define the proportions\n","# We'll reserve 15% of the labeled data for the test set.\n","test_frac = 0.15\n","\n","# Split off the test set\n","labeled_remaining, test_df = train_test_split(\n","    labeled_df,\n","    test_size=test_frac,\n","    stratify=labeled_df[\"stratify_group\"],\n","    random_state=42\n",")\n","\n","# Now, we want the training set to be 70% of the labeled data and validation 15%.\n","# Since we already took out 15% for test, the remaining 85% should be split into:\n","# Training: 70/85 ≈ 0.8235 of the remaining data\n","# Validation: 15/85 ≈ 0.1765 of the remaining data\n","\n","train_frac = 0.8235  # approximately 70% overall\n","labeled_remaining[\"stratify_group\"] = labeled_remaining[\"type\"].astype(str) + \"_\" + labeled_remaining[\"label\"].astype(str)\n","train_df, val_df = train_test_split(\n","    labeled_remaining,\n","    test_size=(1 - train_frac),  # remaining fraction for validation\n","    stratify=labeled_remaining[\"stratify_group\"],\n","    random_state=42\n",")\n","\n","# Drop the helper stratification column from all splits\n","for df in [train_df, val_df, test_df]:\n","    df.drop(columns=[\"stratify_group\"], inplace=True)\n","\n","# Function to print counts with percentages\n","def print_distribution(df, column_name, set_name):\n","    print(f\"\\n{set_name} distribution for {column_name}:\")\n","    counts = df[column_name].value_counts()\n","    percentages = df[column_name].value_counts(normalize=True) * 100\n","    for category in counts.index:\n","        print(f\"  {category}: {counts[category]} ({percentages[category]:.2f}%)\")\n","\n","# Print shapes and distributions for each split\n","print(\"Training set shape:\", train_df.shape)\n","print(\"Validation set shape:\", val_df.shape)\n","print(\"Test set shape:\", test_df.shape)\n","\n","print_distribution(train_df, \"label\", \"Training\")\n","print_distribution(train_df, \"type\", \"Training\")\n","\n","print_distribution(val_df, \"label\", \"Validation\")\n","print_distribution(val_df, \"type\", \"Validation\")\n","\n","print_distribution(test_df, \"label\", \"Test\")\n","print_distribution(test_df, \"type\", \"Test\")\n","\n","# Optionally, save the splits for further processing\n","train_df.to_csv(\"train_70pct.csv\", index=False)\n","val_df.to_csv(\"validation_15pct.csv\", index=False)\n","test_df.to_csv(\"test_15pct.csv\", index=False)\n","print(\"\\nSplits saved as 'train_70pct.csv', 'validation_15pct.csv', and 'test_15pct.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WVcYyzYv0gr","executionInfo":{"status":"ok","timestamp":1741235906688,"user_tz":300,"elapsed":5076,"user":{"displayName":"Sadra Teymourian","userId":"07534993455976504565"}},"outputId":"6956fa48-2ace-45dd-f0f6-39a6372ead06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-adaccf334cff>:5: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  labeled_df = pd.read_csv(\"labeled_20pct.csv\")\n"]},{"output_type":"stream","name":"stdout","text":["Training set shape: (139980, 46)\n","Validation set shape: (30002, 46)\n","Test set shape: (29997, 46)\n","\n","Training distribution for label:\n","  1: 134945 (96.40%)\n","  0: 5035 (3.60%)\n","\n","Training distribution for type:\n","  scanning: 44739 (31.96%)\n","  ddos: 38610 (27.58%)\n","  dos: 21131 (15.10%)\n","  xss: 13205 (9.43%)\n","  password: 10771 (7.69%)\n","  normal: 5035 (3.60%)\n","  backdoor: 3181 (2.27%)\n","  injection: 2840 (2.03%)\n","  ransomware: 460 (0.33%)\n","  mitm: 8 (0.01%)\n","\n","Validation distribution for label:\n","  1: 28923 (96.40%)\n","  0: 1079 (3.60%)\n","\n","Validation distribution for type:\n","  scanning: 9589 (31.96%)\n","  ddos: 8275 (27.58%)\n","  dos: 4529 (15.10%)\n","  xss: 2830 (9.43%)\n","  password: 2309 (7.70%)\n","  normal: 1079 (3.60%)\n","  backdoor: 682 (2.27%)\n","  injection: 609 (2.03%)\n","  ransomware: 99 (0.33%)\n","  mitm: 1 (0.00%)\n","\n","Test distribution for label:\n","  1: 28918 (96.40%)\n","  0: 1079 (3.60%)\n","\n","Test distribution for type:\n","  scanning: 9587 (31.96%)\n","  ddos: 8274 (27.58%)\n","  dos: 4528 (15.09%)\n","  xss: 2830 (9.43%)\n","  password: 2308 (7.69%)\n","  normal: 1079 (3.60%)\n","  backdoor: 682 (2.27%)\n","  injection: 609 (2.03%)\n","  ransomware: 99 (0.33%)\n","  mitm: 1 (0.00%)\n","\n","Splits saved as 'train_70pct.csv', 'validation_15pct.csv', and 'test_15pct.csv'.\n"]}]}]}